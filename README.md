# ğŸš¦ Automated Annual Air Quality Monitoring for Bandung

## ğŸ“„ Overview

Proyek ini mengimplementasikan sebuah pipeline data end-to-end untuk memonitor kualitas udara tahunan (PM2.5) di Bandung. Dimulai dengan data historis dari laporan publik sebagai fondasi, arsitektur ini dirancang untuk dapat diperluas dengan sumber data dinamis seperti API di masa depan. Tujuan utamanya adalah mengotomatiskan analisis tren jangka panjang untuk mendukung kebijakan lingkungan dan memberdayakan masyarakat dengan data yang mudah diakses.

Pipeline ini diorkestrasi oleh Apache Airflow, menggunakan Python untuk ekstraksi data ke Neon DB (PostgreSQL) yang berfungsi sebagai staging area dan data warehouse. Transformasi dan agregasi data dijalankan oleh Apache Spark, dengan hasil akhir yang divisualisasikan pada dashboard Streamlit interaktif. Platform ini juga dirancang untuk dapat mengirimkan notifikasi otomatis melalui Email atau Telegram jika terdeteksi tingkat polusi yang melebihi ambang batas.
* kak boleh gak ya dilulusin dulu, masihh ingin dikembangin dan benerin karena ini kurang banget, nanti pas dikoreksi masih kurang , aku bakal langsung benerin, tapi benerann kak izin banget buat dilulusin :(
---

## ğŸ“ Project Structure

```plaintext
:\Users\NoXox\Documents\bandung_airbatch/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Kesehatan_Udara_Bandung_2022.xlsx  # Perhatikan: ini adalah file .xlsx, bukan .csv
â”‚   â”œâ”€â”€ Kesehatan_Udara_Bandung_2023.xlsx
â”‚   â”œâ”€â”€ Kesehatan_Udara_Bandung_2024.xlsx
â”‚   â””â”€â”€ Kesehatan_Udara_Bandung_2025.xlsx
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ Dag_Bandung_yearly_air_quality_pipeline.py # Nama file DAG berbeda
â”‚   â”œâ”€â”€ Python Script_extract_local_csv.py         # Nama file script Python berbeda
â”‚   â””â”€â”€ create_table_staging_raw_air_quality.sql   # SQL file ini langsung di dags/, bukan di dags/sql/
â”œâ”€â”€ spark_jobs/
â”œâ”€â”€ streamlit_app/
â”‚   â”œâ”€â”€ Streamlit requirements.txt                 # Nama file requirements Streamlit berbeda
â”‚   â””â”€â”€ Streamlit.py                               # Nama file aplikasi Streamlit berbeda
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ PySpark Job_Bandung_yearly_air_quality.py      # File PySpark ini ada di root, bukan di spark_jobs/
â””â”€â”€ requirements.txt



##Objectives

- Mengkonsolidasikan data kualitas udara tahunan (PM2.5) dari file CSV yang disediakan.
- Menghitung rata-rata tahunan, nilai maksimum, dan mendeteksi outlier untuk metrik polusi.
- Menandai tahun-tahun dengan lonjakan polusi yang tidak biasa berdasarkan logika ambang batas.
- Memvisualisasikan tren jangka panjang via dashboard.
- Memberi tahu pemangku kepentingan secara otomatis melalui email atau Telegram.

ğŸ“š Data Sources

-website databooks (https://databoks.katadata.co.id/layanan-konsumen-kesehatan/statistik/3b72788adeb2920/kualitas-udara-di-kota-besar-indonesia-buruk-jauh-dari-standar-who)
-BMKG API â€“ archived daily JSON data
-data/go.id â€“ downloadable CSV/JSON datasets
-Nafas Indonesia â€“ sensor reports in PDF/Excel format
-IQAir Bandung â€“ scraped AQI and pollutant archive
*untuk sekarang dataset yang saya gunakan dari nafas indonesia

Features
- ğŸŒ Ekstraksi data otomatis dari file CSV yang disediakan (dengan potensi ekspansi ke API dan web scraping di masa depan).
-ğŸ—„ï¸ Staging PostgreSQL dan warehouse terpartisi untuk kueri cepat.
-âš¡ Spark batch jobs untuk menghitung statistik tahunan.
-ğŸ“Š Dashboard Streamlit interaktif dengan filter dan bagan.
-ğŸ”” Peringatan via Telegram/email ketika polusi melebihi ambang batas.
-ğŸ” Penjadwalan yang dapat direproduksi dengan Apache Airflow.
-ğŸ³ Penyiapan Docker Compose untuk deployment.

ğŸ› ï¸ Tech Stack
Component            Tool
-------------------- --------------------------------------
Orchestration        Apache Airflow
Extraction & ETL     Python (requests, pandas, tabula-py)
Batch Processing     Apache Spark
Data Storage         PostgreSQL
Visualization        Streamlit
Alerting             SMTP, Telegram Bot
Containerization     Docker, Docker Compose

[1. Sumber Data Awal]
    (untuk awal sekarang file excel dulu, kedepannya ditingkatin berbagai sumber kak, masih bingung :()
        |
        v
[2. Ekstraksi Data] â€”â€”â€” (Diorkestrasi oleh ğŸ’¨ Airflow)
    (Python Script)
        |
        v
[3. Staging Area]
    (Neon DB (PostgreSQL) ğŸ˜)
        |
        v
[4. Transformasi Data] â€”â€”â€” (Diorkestrasi oleh ğŸ’¨ Airflow)
    (Apache Spark âœ¨)
        |
        v
[5. Data Warehouse]
    (Neon DB (PostgreSQL) ğŸ˜)
        |
        v
[6. Visualisasi & Aksi]
    (Streamlit Dashboard ğŸ“Š)


âš™ï¸âš™ï¸ Pipeline Overview
1. ETL (Airflow DAG yearly_air_quality_pipeline)
Extract: Membaca data dari file CSV tahunan yang disediakan 
Load (Staging): Menyimpan data mentah ke tabel staging.raw_air_quality di PostgreSQL. Analytics Generation (Spark Job)
Transform: Membaca data dari staging, memvalidasi skema, dan melakukan transformasi dasar (misal: mengganti nama kolom).
Load (Warehouse): Menyimpan data matang yang sudah diolah ke tabel warehouse.fact_yearly_air_quality.

Visualization (Streamlit)
Query: Aplikasi Streamlit melakukan kueri langsung ke tabel warehouse di PostgreSQL.
Display: Menampilkan tren kualitas udara tahunan dalam bentuk grafik batang dan tabel interaktif. Koneksi Database (PostgreSQL)

psql 'postgresql://neondb_owner:npg_odbj5JHY0pwO@ep-cold-grass-a18xlnz0-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require'

Keterbatasan & Rencana Pengembangan
Untuk pengembangan di masa depan, terdapat beberapa keterbatasan pada platform saat ini yang bisa menjadi peluang untuk perbaikan:

Keterbatasan Saat Ini (Limitations)
- Dataset statis, belum mendukung incremental load atau streaming.
- Tidak semua field (seperti tanggal transaksi) diproses secara time-series (fokus saat ini pada agregasi tahunan).
- Belum terhubung ke BI tools lain seperti Looker/Power BI.
- Proses ekstraksi data masih bergantung pada file CSV yang disiapkan secara semi-manual.
- Platform hanya berjalan dalam mode batch (tahunan), sehingga cocok untuk analisis historis tetapi tidak untuk pemantauan real-time.
- Dashboard Streamlit menyajikan visualisasi data dasar yang informatif.

Rencana Pengembangan (Recommendations)
- Tambah time dimension untuk analitik per bulan atau per hari, memungkinkan analisis tren yang lebih granular.
- Tambah scheduler untuk export PNG otomatis dari Metabase (atau alat visualisasi lain yang terhubung).
- Sinkronisasi data dari API eksternal (misal BMKG, OpenAQ) untuk mencapai otomatisasi penuh dan memperkaya data.
- Jika kebutuhan bisnis berkembang, platform dapat diperluas dengan menambahkan alur streaming menggunakan teknologi seperti Apache Kafka dan Spark Streaming untuk notifikasi yang lebih cepat.
- Menambahkan fitur analitik yang lebih canggih di masa depan, seperti analisis prediktif (forecasting) untuk memperkirakan kualitas udara atau analisis korelasi dengan data lain (misalnya, data cuaca atau lalu lintas).

ğŸ‘¤ Author
Project by Rafli Firmansyah â€” dibangun untuk tujuan edukasi dan pengembangan portofolio di bidang data engineering.
ğŸ“ License
Proyek ini ditujukan untuk penggunaan edukasi dan portofolio.
